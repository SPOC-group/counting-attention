{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c418f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from memory_exp import *\n",
    "from manual_models import * \n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "def gram_schmidt(vectors):\n",
    "    num_vectors, dim = vectors.shape\n",
    "    ortho_vectors = np.zeros((num_vectors, dim))\n",
    "\n",
    "    for i in range(num_vectors):\n",
    "        ortho_vectors[i] = vectors[i]\n",
    "\n",
    "        for j in range(i):\n",
    "            ortho_vectors[i] -= np.dot(vectors[i], ortho_vectors[j]) / np.dot(ortho_vectors[j], ortho_vectors[j]) * ortho_vectors[j]\n",
    "\n",
    "    # Normalize the vectors\n",
    "    ortho_vectors = np.array([v / np.linalg.norm(v) for v in ortho_vectors])\n",
    "\n",
    "    return ortho_vectors\n",
    "\n",
    "def generate_orthogonal_vectors(n):\n",
    "    random_vectors = np.random.rand(n, n)  # Generate random vectors\n",
    "    orthogonal_vectors = gram_schmidt(random_vectors)\n",
    "\n",
    "    return orthogonal_vectors\n",
    "\n",
    "def relu(x):\n",
    "    return np.where(x<0,0,x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def implement_W2(interval):\n",
    "    L = len(interval)\n",
    "    # if the interval is descending, we need to reverse it\n",
    "    reverted = False\n",
    "    if interval[0] > interval[-1]:\n",
    "        interval = interval[::-1]\n",
    "        reverted = True\n",
    "    ws = np.zeros(L+1)\n",
    "    bs = np.zeros(L+1)\n",
    "    ws[0] = -3\n",
    "    bs[0] = 30\n",
    "    for i, v in enumerate(interval):\n",
    "        i = i+1\n",
    "        ws[i] = ws[i-1] + 0.5\n",
    "        bs[i] = (v * ws[i-1] + bs[i-1]) - (ws[i] * v)\n",
    "        \n",
    "    if reverted:\n",
    "        ws = ws[::-1]\n",
    "        bs = bs[::-1]\n",
    "    return ws, bs\n",
    "\n",
    "def test(model, T , L, has_BOS = True, verbose=False):\n",
    "    works = True\n",
    "    for k in range(1,L+1):\n",
    "        if has_BOS:\n",
    "            x = [T] + [np.random.randint(1,T)] * k + (L-k) * [0]\n",
    "        else:\n",
    "            x = [np.random.randint(1,T)] * k + (L-k) * [0]\n",
    "        x = torch.tensor(x ).to(device).reshape(1,-1)\n",
    "        r = model(x)\n",
    "        if has_BOS:\n",
    "            x_hat = r[0,1].argmax().item()\n",
    "        else:\n",
    "            x_hat = r[0,0].argmax().item()\n",
    "        if x_hat != k:\n",
    "            if verbose:\n",
    "                print(f\"I got {x_hat}, but should have been {k}.\")\n",
    "            works = False\n",
    "    if works:\n",
    "        print(\"Test passed, model works!\")\n",
    "    else:\n",
    "        print(\":((( Test failed, model does not work!\")\n",
    "\n",
    "def generate_perfect_weights(config):\n",
    "    if config['attention_input'] == 'only_sem' and config['no_softmax'] == False and config['dataset_type'] == 'backward':\n",
    "        return generate_perfect_weights_dot_sftm(config)\n",
    "    elif config['attention_input'] == 'linear' and config['no_softmax'] == False and config['dataset_type'] == 'backward':\n",
    "        return generate_perfect_weights_linear_sftm(config)\n",
    "    elif config['attention_input'] == 'linear' and config['no_softmax'] == True and config['dataset_type'] == 'backward':\n",
    "        return generate_perfect_weights_linear(config)\n",
    "    elif config['attention_input'] == 'only_sem' and config['no_softmax'] == False and config['dataset_type'] == 'backward_BOS':\n",
    "        return generate_perfect_weights_bos(config)\n",
    "    elif config['attention_input'] == 'only_sem' and config['no_softmax'] == True and config['dataset_type'] == 'backward_BOS':\n",
    "        return generate_perfect_weights_bos_nsftm(config)\n",
    "    elif config['attention_input'] == 'only_sem' and config['no_softmax'] == True and config['dataset_type'] == 'backward':\n",
    "        \"\"\"Backward counter\"\"\"\n",
    "        return generate_perfect_weights_dot_nosftm(config)\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot generate perfect weights for {config['attention_input']}\")\n",
    "    \n",
    "def generate_perfect_weights_dot_nosftm(config):\n",
    "    assert config['T'] == config['model_dim']\n",
    "    assert config['p'] == 1\n",
    "    \n",
    "    L = config['seq_len']\n",
    "    T = config['T']\n",
    "    has_BOS =  'BOS' in config['dataset_type']\n",
    "    \n",
    "    \n",
    "\n",
    "    model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                                            model_dim=config['model_dim'],\n",
    "                                            p=config['p'],n_classes=config['seq_len']+1,\n",
    "                                            L=config['seq_len'],\n",
    "                                            attention_input=config['attention_input'],use_softmax=not config['no_softmax']).to(device)\n",
    "\n",
    "    word_embeddings_basis = generate_orthogonal_vectors(config['model_dim'])\n",
    "    counter = word_embeddings_basis.sum(axis=0)\n",
    "    word_embeddings = word_embeddings_basis + counter[None,:]\n",
    "    \n",
    "    model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "\n",
    "    model.token_mixer.Q.data = torch.eye(config['model_dim']).to(device) * np.sqrt(config['model_dim'])\n",
    "    model.token_mixer.K.data = torch.eye(config['model_dim']).to(device)\n",
    "    \n",
    "    W_1 = counter.reshape(-1,1).repeat(1,1).T  / (T+1)\n",
    "    b_1 = 0\n",
    "    model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "    model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "    \n",
    "    softmax_table = np.arange(1,L+1) + L*(T+2)+1\n",
    "    descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2 #* 30\n",
    "\n",
    "    ws, bs = implement_W2(descision_boundaries)\n",
    "    # weight 0 is always smallest\n",
    "    ws = np.array([-100] + list(ws))\n",
    "    bs = np.array([-100] + list(bs))\n",
    "    W_2, b_2 = ws, bs\n",
    "    W_2 = W_2[:,None].repeat(1,1)\n",
    "    model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "    model.fc2.bias.data = torch.tensor(b_2).float().to(device)\n",
    "    return model, ws, bs, softmax_table, descision_boundaries\n",
    "    \n",
    "    \n",
    "def generate_perfect_weights_bos(config):\n",
    "    assert config['T'] == config['model_dim']\n",
    "    assert config['p'] == 1\n",
    "    T = config['T']\n",
    "    \n",
    "    L = config['seq_len']\n",
    "    has_BOS =  'BOS' in config['dataset_type']\n",
    "    \n",
    "\n",
    "    model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                                            model_dim=config['model_dim'],\n",
    "                                            p=config['p'],n_classes=config['seq_len']+1,\n",
    "                                            L=config['seq_len'],\n",
    "                                            attention_input=config['attention_input'],use_softmax=not config['no_softmax']).to(device)\n",
    "\n",
    "    word_embeddings_basis = generate_orthogonal_vectors(config['model_dim'])\n",
    "    BOS = word_embeddings_basis.sum(axis=0)\n",
    "    word_embeddings = np.concatenate([word_embeddings_basis, BOS[None,:]])\n",
    "\n",
    "    model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "\n",
    "    model.token_mixer.Q.data = torch.eye(config['model_dim']).to(device) * np.sqrt(config['model_dim'])\n",
    "    model.token_mixer.K.data = torch.eye(config['model_dim']).to(device)\n",
    "\n",
    "    softmax_table = []\n",
    "    for i in range(1,config['seq_len']+1):\n",
    "        x = [T] + [0] * i + (L-i) * [1]\n",
    "        x = torch.tensor(x ).to(device).reshape(1,-1)\n",
    "        # load attn matrix\n",
    "        r = model(x)\n",
    "        attn_probs = model.attn_probs.cpu().detach().numpy()\n",
    "        print(i, attn_probs[0,1,0], attn_probs[0,1,i])\n",
    "        softmax_table.append(attn_probs[0,1,0] * (T - 1) + 2)\n",
    "\n",
    "    W_1 = BOS.reshape(-1,1).repeat(1,1).T \n",
    "    b_1 = 0\n",
    "    model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "    model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "    \n",
    "    softmax_table = np.array(softmax_table)\n",
    "    descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2 #* 30\n",
    "\n",
    "    ws, bs = implement_W2(descision_boundaries)\n",
    "    # weight 0 is always smallest\n",
    "    ws = np.array([-100] + list(ws))\n",
    "    bs = np.array([-100] + list(bs))\n",
    "    W_2, b_2 = ws, bs\n",
    "    W_2 = W_2[:,None].repeat(1,1)\n",
    "    model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "    model.fc2.bias.data = torch.tensor(b_2).float().to(device)\n",
    "    return model, ws, bs, softmax_table, descision_boundaries\n",
    "\n",
    "def generate_perfect_weights_bos_nsftm(config):\n",
    "    assert config['T'] == config['model_dim']\n",
    "    assert config['p'] == 1\n",
    "    T = config['T']\n",
    "    \n",
    "    L = config['seq_len']\n",
    "    has_BOS =  'BOS' in config['dataset_type']\n",
    "    \n",
    "\n",
    "    model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                                            model_dim=config['model_dim'],\n",
    "                                            p=config['p'],n_classes=config['seq_len']+1,\n",
    "                                            L=config['seq_len'],\n",
    "                                            attention_input=config['attention_input'],use_softmax=not config['no_softmax']).to(device)\n",
    "\n",
    "    word_embeddings_basis = generate_orthogonal_vectors(config['model_dim'])\n",
    "    BOS = word_embeddings_basis.sum(axis=0)\n",
    "    word_embeddings = np.concatenate([word_embeddings_basis, BOS[None,:]])\n",
    "\n",
    "    model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "\n",
    "    model.token_mixer.Q.data = torch.eye(config['model_dim']).to(device) * np.sqrt(config['model_dim'])\n",
    "    model.token_mixer.K.data = torch.eye(config['model_dim']).to(device)\n",
    "\n",
    "    softmax_table = []\n",
    "    for i in range(1,config['seq_len']+1):\n",
    "        x = [T] + [0] * i + (L-i) * [1]\n",
    "        x = torch.tensor(x ).to(device).reshape(1,-1)\n",
    "        # load attn matrix\n",
    "        r = model(x)\n",
    "        attn_probs = model.attn_probs.cpu().detach().numpy()\n",
    "        softmax_table.append(attn_probs[0,1,0] * T + attn_probs[0,1,1:i+1].sum())\n",
    "\n",
    "    W_1 = BOS.reshape(-1,1).repeat(1,1).T \n",
    "    b_1 = -1.0\n",
    "    model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "    model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "    \n",
    "    softmax_table = np.array(softmax_table)\n",
    "    descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2 #* 30\n",
    "\n",
    "    ws, bs = implement_W2(descision_boundaries)\n",
    "    # weight 0 is always smallest\n",
    "    ws = np.array([-100] + list(ws))\n",
    "    bs = np.array([-100] + list(bs))\n",
    "    W_2, b_2 = ws, bs\n",
    "    W_2 = W_2[:,None].repeat(1,1)\n",
    "    model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "    model.fc2.bias.data = torch.tensor(b_2).float().to(device)\n",
    "    return model, ws, bs, softmax_table, descision_boundaries\n",
    "\n",
    "def generate_perfect_weights_linear_sftm(config):\n",
    "    assert config['T'] == config['model_dim'] == config['p']\n",
    "    has_BOS =  'BOS' in config['dataset_type']\n",
    "    L = config['seq_len'] \n",
    "\n",
    "    model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                                model_dim=config['model_dim'],\n",
    "                                p=config['p'],n_classes=config['seq_len']+1,\n",
    "                                L=config['seq_len'],\n",
    "                                attention_input=config['attention_input'],\n",
    "                                use_softmax=not config['no_softmax']).to(device)\n",
    "    \n",
    "    word_embeddings_basis = generate_orthogonal_vectors(config['model_dim'])\n",
    "\n",
    "    word_embeddings = word_embeddings_basis \n",
    "    model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "    \n",
    "    a = 1/(config['seq_len']+1)\n",
    "    A = a * np.ones((config['seq_len'],config['seq_len'])) + np.eye(config['seq_len']) * a\n",
    "    model.token_mixer.A.data = torch.tensor(A).float().to(device)\n",
    "    \n",
    "    softmax_table = []\n",
    "    for i in range(1,config['seq_len']+1):\n",
    "        x = [0] * i + (L-i) * [1]\n",
    "        x = torch.tensor(x ).to(device).reshape(1,-1)\n",
    "        # load attn matrix\n",
    "        r = model(x)\n",
    "        attn_probs = model.attn_probs.cpu().detach().numpy()\n",
    "        softmax_table.append(attn_probs[0,0,0:i].sum())\n",
    " \n",
    "    W_1 = torch.tensor(word_embeddings).float().to(device)\n",
    "    b_1 = - 1.0\n",
    "    model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "    model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "    softmax_table = np.array(softmax_table)\n",
    "    descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2\n",
    "    ws, bs = implement_W2(descision_boundaries)\n",
    "    # weight 0 is always smallest\n",
    "    ws = np.array([-100] + list(ws))\n",
    "    bs = np.array([-100] + list(bs))\n",
    "    W_2, b_2 = ws, bs\n",
    "    W_2 = W_2[:,None].repeat(config['T'],1)\n",
    "    model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "    model.fc2.bias.data = torch.tensor(b_2).float().to(device)\n",
    "    return model, ws, bs, softmax_table, descision_boundaries\n",
    "\n",
    "   \n",
    "def generate_perfect_weights_linear(config):\n",
    "    assert config['T'] == config['model_dim'] == config['p']\n",
    "    has_BOS =  'BOS' in config['dataset_type']\n",
    "    L = config['seq_len'] \n",
    "\n",
    "    model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                                model_dim=config['model_dim'],\n",
    "                                p=config['p'],n_classes=config['seq_len']+1,\n",
    "                                L=config['seq_len'],\n",
    "                                attention_input=config['attention_input'],\n",
    "                                use_softmax=not config['no_softmax']).to(device)\n",
    "    \n",
    "    word_embeddings_basis = generate_orthogonal_vectors(config['model_dim'])\n",
    "\n",
    "    word_embeddings = word_embeddings_basis \n",
    "    model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "    \n",
    "    a = 1/(config['seq_len']+1)\n",
    "    \n",
    "    A = a * np.ones((config['seq_len'],config['seq_len'])) + np.eye(config['seq_len']) * a\n",
    "    \n",
    "    model.token_mixer.A.data = torch.tensor(A).float().to(device)\n",
    "    W_1 = torch.tensor(word_embeddings).float().to(device)\n",
    "    b_1 = - (1.0+a)\n",
    "    model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "    model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "\n",
    "    \n",
    "    softmax_table = np.cumsum(np.ones(config['seq_len']+1) * a)\n",
    "    descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2\n",
    "    ws, bs = implement_W2(descision_boundaries)\n",
    "    # weight 0 is always smallest\n",
    "    ws = np.array([-100] + list(ws))\n",
    "    bs = np.array([-100] + list(bs))\n",
    "    W_2, b_2 = ws, bs\n",
    "    W_2 = W_2[:,None].repeat(config['T'],1)\n",
    "    model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "    model.fc2.bias.data = torch.tensor(b_2).float().to(device)\n",
    "    return model, ws, bs, softmax_table, descision_boundaries\n",
    "        \n",
    "def generate_perfect_weights_dot_sftm(config):\n",
    "    assert config['T'] == config['model_dim'] == config['p']\n",
    "    \n",
    "    L = config['seq_len']\n",
    "    has_BOS =  'BOS' in config['dataset_type']\n",
    "\n",
    "    model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                                model_dim=config['model_dim'],\n",
    "                                p=config['p'],n_classes=config['seq_len']+1,\n",
    "                                L=config['seq_len'],\n",
    "                                attention_input=config['attention_input'],\n",
    "                                use_softmax=not config['no_softmax']).to(device)\n",
    "\n",
    "    word_embeddings_basis = generate_orthogonal_vectors(config['model_dim'])\n",
    "\n",
    "    word_embeddings = word_embeddings_basis \n",
    "    model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "\n",
    "    model.token_mixer.Q.data = torch.eye(config['model_dim']).to(device) * np.sqrt(config['model_dim'])\n",
    "    model.token_mixer.K.data = torch.eye(config['model_dim']).to(device)\n",
    "\n",
    "    softmax_table = []\n",
    "    for i in range(1,config['seq_len']+1):\n",
    "        x = [0] * i + (L-i) * [1]\n",
    "        x = torch.tensor(x ).to(device).reshape(1,-1)\n",
    "        # load attn matrix\n",
    "        r = model(x)\n",
    "        attn_probs = model.attn_probs.cpu().detach().numpy()\n",
    "        softmax_table.append(attn_probs[0,0,0]*i)\n",
    "    \n",
    "    W_1 = torch.tensor(word_embeddings).float().to(device)\n",
    "    b_1 = -1.0\n",
    "    model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "    model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "    \n",
    "    softmax_table = np.array(softmax_table)\n",
    "    descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2\n",
    "    ws, bs = implement_W2(descision_boundaries)\n",
    "    # weight 0 is always smallest\n",
    "    ws = np.array([-100] + list(ws))\n",
    "    bs = np.array([-100] + list(bs))\n",
    "    W_2, b_2 = ws, bs\n",
    "    W_2 = W_2[:,None].repeat(config['T'],1)\n",
    "    model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "    model.fc2.bias.data = torch.tensor(b_2).float().to(device)\n",
    "    return model, ws, bs, softmax_table, descision_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3093fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 32\n",
    "L = 10\n",
    "assert T>L\n",
    "figure_dir = FIGURE_DIR / 'paper_plots'\n",
    "figure_dir.mkdir(exist_ok=True,parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3b7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bos+sftm\n",
    "\n",
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': int(np.ceil(np.log2(T + 1)) + 2),\n",
    "    'p': 1,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'only_sem',\n",
    "    'no_softmax': False,\n",
    "    'dataset_type': 'backward_BOS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bec012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_BOS =  'BOS' in config['dataset_type']\n",
    "L = config['seq_len'] \n",
    "d = config['model_dim']\n",
    "model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                            model_dim=config['model_dim'],\n",
    "                            p=config['p'],n_classes=config['seq_len']+1,\n",
    "                            L=config['seq_len'],\n",
    "                            attention_input=config['attention_input'],\n",
    "                            use_softmax=not config['no_softmax']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d534d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "kappa = 200\n",
    "d_prime = d-2\n",
    "numbers = np.arange(1, 2**d_prime)\n",
    "E = np.array([list(np.binary_repr(num, width=d_prime)) for num in numbers]).astype(int)\n",
    "E = E / np.sqrt(np.diag(E @ E.T)[:,np.newaxis])\n",
    "E = np.hstack([E,alpha * np.ones((E.shape[0],1)),np.zeros((E.shape[0],1))])\n",
    "eBOS = np.zeros(d)\n",
    "eBOS[-1] = 1\n",
    "eBOS[-2] = 1/alpha\n",
    "word_embeddings = np.concatenate([E[:T], eBOS[None,:]])\n",
    "model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "model.token_mixer.Q.data = kappa * torch.eye(config['model_dim']).to(device) * np.sqrt(config['model_dim'])\n",
    "model.token_mixer.K.data = torch.eye(config['model_dim']).to(device)\n",
    "eCount = np.zeros(d)\n",
    "eCount[-1] = 1\n",
    "W_1 = eCount.reshape(-1,1).repeat(1,1).T \n",
    "b_1 = 0.0\n",
    "model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "softmax_table = []\n",
    "for i in range(1,config['seq_len']+1):\n",
    "    x = [T] + [0] * i + (L-i) * [1]\n",
    "    x = torch.tensor(x).to(device).reshape(1,-1)\n",
    "    r = model(x)\n",
    "    attn_probs = model.attn_probs.cpu().detach().numpy()\n",
    "    softmax_table.append(1-attn_probs[0,1,0]*i)\n",
    "softmax_table = np.array(softmax_table)\n",
    "descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2 #* 30\n",
    "ws, bs = implement_W2(descision_boundaries)\n",
    "ws = np.array([-100] + list(ws))\n",
    "bs = np.array([-100] + list(bs))\n",
    "W_2, b_2 = ws, bs\n",
    "W_2 = W_2[:,None].repeat(1,1)\n",
    "model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "model.fc2.bias.data = torch.tensor(b_2).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d1ffb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bb0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000  samples tested.\n",
      "Test passed, model works!\n"
     ]
    }
   ],
   "source": [
    "res = test(model, T, L, has_BOS='BOS' in config['dataset_type'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bos+sftm\n",
    "\n",
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': 4,\n",
    "    'p': 1,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'only_sem',\n",
    "    'no_softmax': False,\n",
    "    'dataset_type': 'backward_BOS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a419f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_BOS =  'BOS' in config['dataset_type']\n",
    "L = config['seq_len'] \n",
    "d = config['model_dim']\n",
    "model = TransformerSeq2Seq(T=config['T']+int(has_BOS),\n",
    "                            model_dim=config['model_dim'],\n",
    "                            p=config['p'],n_classes=config['seq_len']+1,\n",
    "                            L=config['seq_len'],\n",
    "                            attention_input=config['attention_input'],\n",
    "                            use_softmax=not config['no_softmax']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54f94562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha=0.00001\n",
    "kappa = 30000\n",
    "numbers = np.arange(T).reshape(-1,1)\n",
    "E = np.hstack([np.sqrt(numbers/T),np.sqrt((T-numbers)/T)])\n",
    "E = np.hstack([E,alpha * np.ones((E.shape[0],1)),np.zeros((E.shape[0],1))])\n",
    "eBOS = np.zeros(d)\n",
    "eBOS[-1] = 1\n",
    "eBOS[-2] = 1/alpha\n",
    "word_embeddings = np.concatenate([E[:T], eBOS[None,:]])\n",
    "model.semantic_emb.weight.data = torch.tensor(word_embeddings).float().to(device)\n",
    "eCount = np.zeros(d)\n",
    "eCount[-1] = 1\n",
    "W_1 = eCount.reshape(-1,1).repeat(1,1).T \n",
    "b_1 = 0.0\n",
    "model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n",
    "model.fc1.bias.data = torch.tensor(b_1).float().to(device)\n",
    "model.token_mixer.Q.data = kappa * torch.eye(config['model_dim']).to(device) * np.sqrt(config['model_dim'])\n",
    "model.token_mixer.K.data = torch.eye(config['model_dim']).to(device)\n",
    "softmax_table = []\n",
    "for i in range(1,config['seq_len']+1):\n",
    "    x = [T] + [0] * i + (L-i) * [1]\n",
    "    x = torch.tensor(x).to(device).reshape(1,-1)\n",
    "    r = model(x)\n",
    "    attn_probs = model.attn_probs.cpu().detach().numpy()\n",
    "    softmax_table.append(1-attn_probs[0,1,0]*i)\n",
    "softmax_table = np.array(softmax_table)\n",
    "descision_boundaries = softmax_table[:-1] + (softmax_table[1:] - softmax_table[:-1])/2 #* 30\n",
    "\n",
    "ws, bs = implement_W2(descision_boundaries)\n",
    "ws = np.array([-100] + list(ws))\n",
    "bs = np.array([-100] + list(bs))\n",
    "W_2, b_2 = ws, bs\n",
    "W_2 = W_2[:,None].repeat(1,1)\n",
    "model.fc2.weight.data = torch.tensor(W_2).float().to(device)\n",
    "model.fc2.bias.data = torch.tensor(b_2).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ca022a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000  samples tested.\n",
      "Test passed, model works!\n"
     ]
    }
   ],
   "source": [
    "res = test(model, T, L, has_BOS='BOS' in config['dataset_type'], verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
