{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_import import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from plotting import *\n",
    "from manual_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of manual weight settings for $d = T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 32\n",
    "L = 10\n",
    "assert T>L\n",
    "figure_dir = FIGURE_DIR / 'paper_plots'\n",
    "figure_dir.mkdir(exist_ok=True,parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear (noSFTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m: T,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: T,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m model, ws, bs, precise_values, desicion_bounds \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_perfect_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m test(model, T , L, has_BOS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBOS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m],verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Downloads/counting-attention-main/manual_models.py:86\u001b[0m, in \u001b[0;36mgenerate_perfect_weights\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_perfect_weights_linear_sftm(config)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_input\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_softmax\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_perfect_weights_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_input\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly_sem\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_softmax\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward_BOS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_perfect_weights_bos(config)\n",
      "File \u001b[0;32m~/Downloads/counting-attention-main/manual_models.py:296\u001b[0m, in \u001b[0;36mgenerate_perfect_weights_linear\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    293\u001b[0m has_BOS \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBOS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    294\u001b[0m L \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m--> 296\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerSeq2Seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhas_BOS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmodel_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseq_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseq_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattention_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_softmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mno_softmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    303\u001b[0m word_embeddings_basis \u001b[38;5;241m=\u001b[39m generate_orthogonal_vectors(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    305\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m word_embeddings_basis \n",
      "File \u001b[0;32m~/Downloads/counting-attention-main/memory_exp.py:264\u001b[0m, in \u001b[0;36mTransformerSeq2Seq.__init__\u001b[0;34m(self, T, model_dim, p, n_classes, L, attention_input, use_softmax)\u001b[0m\n\u001b[1;32m    261\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_emb\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_input  \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 264\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_mixer \u001b[38;5;241m=\u001b[39m \u001b[43mLinearMixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_mixer \u001b[38;5;241m=\u001b[39m DotPMixer(model_dim,L,attention_input\u001b[38;5;241m=\u001b[39mattention_input)\n",
      "File \u001b[0;32m~/Downloads/counting-attention-main/memory_exp.py:240\u001b[0m, in \u001b[0;36mLinearMixer.__init__\u001b[0;34m(self, model_dim, seq_len)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dim \u001b[38;5;241m=\u001b[39m model_dim\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len \u001b[38;5;241m=\u001b[39m seq_len\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    241\u001b[0m nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mkaiming_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mT, a\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': T,\n",
    "    'p': T,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'linear',\n",
    "    'no_softmax': True,\n",
    "    'dataset_type': 'backward'\n",
    "}\n",
    "model, ws, bs, precise_values, desicion_bounds = generate_perfect_weights(config)\n",
    "test(model, T , L, has_BOS = 'BOS' in config['dataset_type'],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear +SFTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, model works!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbehrens/University/counting-attention-arxiv/manual_models.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': T,\n",
    "    'p': T,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'linear',\n",
    "    'no_softmax': False,\n",
    "    'dataset_type': 'backward'\n",
    "}\n",
    "model, ws, bs, precise_values, desicion_bounds = generate_perfect_weights(config)\n",
    "test(model, T , L, has_BOS = 'BOS' in config['dataset_type'],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear + SFTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, model works!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': T,\n",
    "    'p': T,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'linear',\n",
    "    'no_softmax': True,\n",
    "    'dataset_type': 'backward'\n",
    "}\n",
    "model, ws, bs, precise_values, desicion_bounds = generate_perfect_weights(config)\n",
    "test(model, T , L, has_BOS = 'BOS' in config['dataset_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOT (noSFTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, model works!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': T,\n",
    "    'p': 1,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'only_sem',\n",
    "    'no_softmax': True,\n",
    "    'dataset_type': 'backward'\n",
    "}\n",
    "model, ws, bs, precise_values, desicion_bounds = generate_perfect_weights(config)\n",
    "test(model, T , L, has_BOS = 'BOS' in config['dataset_type'],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOT + SFTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, model works!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbehrens/University/counting-attention-arxiv/manual_models.py:363: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model.fc1.weight.data = torch.tensor(W_1).float().to(device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': T,\n",
    "    'p': T,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'only_sem',\n",
    "    'no_softmax': False,\n",
    "    'dataset_type': 'backward'\n",
    "}\n",
    "model, ws, bs, precise_values, desicion_bounds = generate_perfect_weights(config)\n",
    "test(model, T , L, has_BOS = 'BOS' in config['dataset_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOS (nSFTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, model works!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': T,\n",
    "    'p': 1,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'only_sem',\n",
    "    'no_softmax': True,\n",
    "    'dataset_type': 'backward_BOS'\n",
    "}\n",
    "model, ws, bs, precise_values, desicion_bounds = generate_perfect_weights(config)\n",
    "test(model, T , L, has_BOS = 'BOS' in config['dataset_type'],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOS+SFTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.18829146 0.18829146\n",
      "2 0.16826417 0.16826417\n",
      "3 0.15208763 0.15208763\n",
      "4 0.13874865 0.13874865\n",
      "5 0.12756082 0.12756082\n",
      "6 0.1180426 0.1180426\n",
      "7 0.1098462 0.1098462\n",
      "8 0.10271413 0.10271413\n",
      "9 0.09645174 0.09645174\n",
      "10 0.09090909 0.09090909\n",
      "Test passed, model works!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'T': T,\n",
    "    'model_dim': T,\n",
    "    'p': 1,\n",
    "    'seq_len':L,\n",
    "    'attention_input': 'only_sem',\n",
    "    'no_softmax': False,\n",
    "    'dataset_type': 'backward_BOS'\n",
    "}\n",
    "model, ws, bs, precise_values, desicion_bounds = generate_perfect_weights(config)\n",
    "test(model, T , L, has_BOS = 'BOS' in config['dataset_type'],verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
