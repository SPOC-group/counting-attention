{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def generate_vectors(T, d, M, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Generate T unit-normed vectors in d dimensions with mutual coherence M.\n",
    "\n",
    "    Parameters:\n",
    "    T (int): Number of vectors to generate.\n",
    "    d (int): Dimension of each vector.\n",
    "    M (float): Desired mutual coherence.\n",
    "    max_iter (int): Maximum number of iterations to adjust vectors.\n",
    "    tol (float): Tolerance for mutual coherence convergence.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array of shape (T, d) containing the unit-normed vectors.\n",
    "    \"\"\"\n",
    "    # Initialize T random unit vectors\n",
    "    vectors = np.random.randn(T, d)\n",
    "    vectors /= np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    \n",
    "    for iteration in tqdm(range(max_iter)):\n",
    "        # Compute the Gram matrix (inner products between vectors)\n",
    "        G = np.dot(vectors, vectors.T)\n",
    "        np.fill_diagonal(G, 0)  # Exclude self-inner products\n",
    "        \n",
    "        # Find the maximum absolute inner product (mutual coherence)\n",
    "        max_coherence = np.max(np.abs(G))\n",
    "        \n",
    "        if max_coherence <= M + tol:\n",
    "            print(f\"Converged at iteration {iteration}, mutual coherence: {max_coherence}\")\n",
    "            return vectors\n",
    "        else:\n",
    "            # Adjust pairs of vectors that exceed the mutual coherence M\n",
    "            for i in range(T):\n",
    "                for j in range(i + 1, T):\n",
    "                    inner_product = np.dot(vectors[i], vectors[j])\n",
    "                    if np.abs(inner_product) > M:\n",
    "                        # Compute the correction factor\n",
    "                        correction = (inner_product - np.sign(inner_product) * M) / 2\n",
    "                        # Adjust the vectors\n",
    "                        vectors[i] -= correction * vectors[j]\n",
    "                        vectors[j] -= correction * vectors[i]\n",
    "                        # Re-normalize the vectors to unit length\n",
    "                        vectors[i] /= np.linalg.norm(vectors[i])\n",
    "                        vectors[j] /= np.linalg.norm(vectors[j])\n",
    "    print(f\"Did not converge after {max_iter} iterations, mutual coherence: {max_coherence}\")\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def get_mutual_coherence(vectors):\n",
    "    \"\"\"\n",
    "    Compute the mutual coherence of a set of vectors.\n",
    "\n",
    "    Parameters:\n",
    "    vectors (numpy.ndarray): An array of shape (T, d) containing the unit-normed vectors.\n",
    "\n",
    "    Returns:\n",
    "    float: The mutual coherence of the vectors.\n",
    "    \"\"\"\n",
    "    # Compute the Gram matrix (inner products between vectors)\n",
    "    G = np.dot(vectors, vectors.T)\n",
    "    np.fill_diagonal(G, 0)  # Exclude self-inner products\n",
    "    return np.max(np.abs(G))\n",
    "\n",
    "\n",
    "# Each vector represnets a word (a number) in the dictionary. Given a sequence of numbers, we can represent it as a sequence of vectors.\n",
    "def get_embeddings(sequence, vectors):\n",
    "    \"\"\"\n",
    "    Get the embeddings of a sequence of numbers using a set of vectors.\n",
    "\n",
    "    Parameters:\n",
    "    sequence (list): A list of numbers representing the sequence.\n",
    "    vectors (numpy.ndarray): An array of shape (T, d) containing the unit-normed vectors.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array of shape (len(sequence), d) containing the embeddings of the sequence.\n",
    "    \"\"\"\n",
    "    return vectors[sequence-1]\n",
    "\n",
    "\n",
    "def welch_bound(T, d):\n",
    "    return np.sqrt((T-d)/(d*(T-1)))\n",
    "    \n",
    "\n",
    "def get_condition(T,L,alpha):\n",
    "    lower_bound = (T*((2*L-3)/(1-(2*L-4)*alpha**2))**2)/(T-1 + ((2*L-3)/(1-(2*L-4)*alpha**2))**2)\n",
    "    # take the largest integer closest to the lower bound\n",
    "    return int(np.round(lower_bound))+1\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    L = 10\n",
    "    T = 80\n",
    "    alpha = 1e-2\n",
    "\n",
    "    # key result: Welch bound + consistency condition\n",
    "    minimal_d = get_condition(T,L,alpha)\n",
    "    print(f\"Condition d: {minimal_d}\")\n",
    "\n",
    "    ds = np.arange(int(minimal_d), T,1)\n",
    "    print('ds:', ds)\n",
    "\n",
    "    # create a dataset of sequences to loop over\n",
    "    print('creating dataset')\n",
    "    N = 1000000\n",
    "    sequences = []\n",
    "    for i in tqdm(range(N)):\n",
    "        K=L\n",
    "        T_ = [i for i in range(0, T)]\n",
    "        sequence = [0 for _ in range(L)]\n",
    "        while K > 1:\n",
    "            # sample integer k between 1 and K\n",
    "            k = np.random.randint(1, K+1)\n",
    "            # sample a token between 1 and T_\n",
    "            try:\n",
    "                t = int(np.random.choice(T_))\n",
    "            except ValueError:\n",
    "                breakpoint()\n",
    "            # set x_i = t for i = k,..,K\n",
    "            sequence[k-1:K] = [t for i in range(K-k+1)]\n",
    "            # delete t from T_ and K = k\n",
    "            T_.remove(t)\n",
    "            K = k\n",
    "        # shuffle the sequence\n",
    "        np.random.shuffle(sequence)\n",
    "        sequences.append(sequence)\n",
    "    sequences = np.stack(sequences)+1\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    Ms = []\n",
    "    logits_distributions = []\n",
    "    is_within = []  \n",
    "    is_margin_ok = []\n",
    "    for  d in ds:\n",
    "        #Theory condition of M: M < (1/(2*L-3))-((2*L-4)/(2*L-3))*alpha**2\n",
    "        M_max = (1/(2*L-3))-((2*L-4)/(2*L-3))*alpha**2\n",
    "        M_min  = welch_bound(T, d-1)\n",
    "        print('sampling M between:', M_min, M_max)\n",
    "        M = np.random.uniform(M_min, M_max)\n",
    "        print('desired M:', M)\n",
    "\n",
    "\n",
    "        vecs = generate_vectors(T, d-1, M)\n",
    "        M_est = get_mutual_coherence(vecs)\n",
    "        vecs = np.concatenate((vecs, alpha*np.ones((T,1))), axis=1)\n",
    "        # Check that the inner product of two equal vectors is 1 + alpha^2\n",
    "        assert np.allclose(np.dot(vecs[0], vecs[0]), 1 + alpha**2)\n",
    "        # Check that the absolute inner product of two different vectors is less than M + alpha^2\n",
    "        to_check = np.abs(np.dot(vecs, vecs.T) - np.eye(T))\n",
    "        wh = np.where(to_check > M_est + alpha**2)\n",
    "        if len(wh[0]) != 0:\n",
    "            breakpoint()\n",
    "        # if not np.isclose(M, M_est, atol=1e-2):\n",
    "        #     print('Empirical mutual coherence {} is not close to the desired value {}'.format(M_est, M))\n",
    "        #     print()\n",
    "        #     print()\n",
    "        #     continue\n",
    "        if M_est > M_max:\n",
    "            print('Empirical mutual coherence {} is greater than theoretical max value {}'.format(M_est, M_max))\n",
    "            Ms.append(M_est)\n",
    "            is_within.append(False)\n",
    "        elif M_est < M_min:\n",
    "            print('Empirical mutual coherence {} is smaller than the Welch bound {}'.format(M_est, M_min))\n",
    "            Ms.append(M_est)\n",
    "            is_within.append(False)\n",
    "        else:\n",
    "            print('Empirical mutual coherence {} is within the desired range'.format(M_est))\n",
    "            Ms.append(M)\n",
    "            is_within.append(True)\n",
    "\n",
    "        # for each draw, keep track of the distribution of logits for each possible count number\n",
    "        e_cnt = np.zeros((d))\n",
    "        e_cnt[-1] = 1/alpha\n",
    "        logits = {k: [] for k in range(1, L+1)}\n",
    "\n",
    "        for sequence in tqdm(sequences):\n",
    "            assert np.max(sequence) <= T\n",
    "            assert len(sequence) == L\n",
    "\n",
    "            tokens_un, counts_un = np.unique(sequence, return_counts=True)\n",
    "            counts = np.zeros(L)\n",
    "            for i, t in enumerate(tokens_un):\n",
    "                counts[np.where(sequence == t)] = counts_un[i]\n",
    "            \n",
    "            #model\n",
    "            embeddings = get_embeddings(sequence, vecs)\n",
    "            dot_product = np.dot(embeddings, embeddings.T)\n",
    "            mixed_dot_product = np.dot(dot_product, embeddings) + embeddings\n",
    "            projected = np.dot(mixed_dot_product, e_cnt)\n",
    "            projected = np.maximum(projected, 0)\n",
    "            \n",
    "            #keep track of the logits\n",
    "            for i, k in enumerate(counts):\n",
    "                logits[int(k)].append(projected[i])\n",
    "\n",
    "        #check that the logits are well separated\n",
    "        logits_distributions.append(logits)\n",
    "        ok = True\n",
    "        for k in range(1, L-1):\n",
    "            try:\n",
    "                if np.max(logits[k]) < np.min(logits[k+1]):\n",
    "                    print('logits are well separated for count:', k)\n",
    "                else:\n",
    "                    print('logits are not well separated for count:', k)\n",
    "                    ok = False\n",
    "            except ValueError:\n",
    "                pass\n",
    "        is_margin_ok.append(ok)\n",
    "        print('is_margin_ok:', ok, 'is_within:', is_within[-1])\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "\n",
    "    # sort Ms and logits_distributions and is_within and is_margin_ok according to Ms\n",
    "    Ms = np.array(Ms)\n",
    "    is_within = np.array(is_within)\n",
    "    is_margin_ok = np.array(is_margin_ok)\n",
    "    sort_idx = np.argsort(Ms)\n",
    "    Ms = Ms[sort_idx]\n",
    "    is_within = is_within[sort_idx]\n",
    "    is_margin_ok = is_margin_ok[sort_idx]\n",
    "    logits_distributions = [logits_distributions[i] for i in sort_idx]\n",
    "\n",
    "\n",
    "\n",
    "    #plot the distribution of logits for M\n",
    "    for i, logits in enumerate(logits_distributions):\n",
    "        plt.figure()\n",
    "        for k in range(1, L-1):\n",
    "            plt.hist(logits[k], bins=100, alpha=0.5, label=str(k))\n",
    "        plt.title('M = {}, is_within: {}, is_margin_ok: {}'.format(Ms[i], is_within[i], is_margin_ok[i]))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
